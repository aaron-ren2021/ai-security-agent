"""Experimental Azure AI Search integration built on azure-search-documents SDK."""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, Iterable, List, Optional, Sequence

logger = logging.getLogger(__name__)

try:
	from azure.search.documents import SearchClient
	from azure.search.documents.indexes import SearchIndexClient
	from azure.search.documents.indexes.models import (
		SearchIndex,
		SimpleField,
		SearchableField,
		SearchField,
		SearchFieldDataType,
		VectorSearch,
		VectorSearchProfile,
		HnswAlgorithmConfiguration,
		SemanticConfiguration,
		SemanticSearch,
		SemanticPrioritizedFields,
		SemanticField,
	)
	from azure.search.documents.models import VectorizedQuery
	from azure.core.credentials import AzureKeyCredential
	from azure.identity import DefaultAzureCredential
	from azure.core.exceptions import HttpResponseError, ResourceNotFoundError
	AZURE_SEARCH_AVAILABLE = True
except ImportError:  # pragma: no cover
	SearchClient = None  # type: ignore
	SearchIndexClient = None  # type: ignore
	SearchIndex = None  # type: ignore
	SimpleField = None  # type: ignore
	SearchableField = None  # type: ignore
	SearchField = None  # type: ignore
	SearchFieldDataType = None  # type: ignore
	VectorSearch = None  # type: ignore
	VectorSearchProfile = None  # type: ignore
	HnswAlgorithmConfiguration = None  # type: ignore
	SemanticConfiguration = None  # type: ignore
	SemanticSearch = None  # type: ignore
	SemanticPrioritizedFields = None  # type: ignore
	SemanticField = None  # type: ignore
	VectorizedQuery = None  # type: ignore
	AzureKeyCredential = None  # type: ignore
	DefaultAzureCredential = None  # type: ignore
	HttpResponseError = Exception  # fallback for typing
	ResourceNotFoundError = Exception
	AZURE_SEARCH_AVAILABLE = False

@dataclass
class AzureSearchConfig:
	service_name: str
	api_key: Optional[str] = None
	index_name: str = "documents"
	vector_field_name: str = "content_vector"
	embedding_dimensions: int = 1536
	analyzer_name: Optional[str] = None
	use_semantic_search: bool = False
	semantic_configuration_name: str = "default-semantic-config"


class AzureAISearchExperimental:
	"""Thin wrapper around the Azure AI Search SDK for experimental flows."""

	def __init__(self, config: AzureSearchConfig):
		if not AZURE_SEARCH_AVAILABLE:
			raise RuntimeError("azure-search-documents SDK is not installed.")

		self.config = config
		endpoint = f"https://{config.service_name}.search.windows.net"

		# Use API key if provided; otherwise attempt DefaultAzureCredential (keyless).
		if getattr(config, 'api_key', None):
			self.credential = AzureKeyCredential(config.api_key)
		else:
			# Use DefaultAzureCredential (MS Entra) if no API key is supplied.
			# This enables keyless authentication flows in production.
			self.credential = DefaultAzureCredential()
			logger.info("Using DefaultAzureCredential for Azure AI Search authentication (keyless)")

		self.search_client = SearchClient(
			endpoint=endpoint,
			index_name=config.index_name,
			credential=self.credential,
		)
		self.index_client = SearchIndexClient(
			endpoint=endpoint,
			credential=self.credential,
		)

	def ensure_index(self, recreate: bool = False) -> None:
		"""Ensure that the Azure AI Search index exists."""
		try:
			if recreate:
				self.index_client.delete_index(self.config.index_name)
				logger.info("Deleted Azure AI Search index %s", self.config.index_name)
		except ResourceNotFoundError:
			pass
		except HttpResponseError as exc:
			logger.error("Failed to delete index %s: %s", self.config.index_name, exc)
			raise

		if self._index_exists():
			return

		try:
			index = self._build_index()
			# Use create_or_update_index so deployments are idempotent.
			self.index_client.create_or_update_index(index)
			logger.info("Created/Updated Azure AI Search index %s", self.config.index_name)
		except HttpResponseError as exc:
			logger.error("Failed to create/update index %s: %s", self.config.index_name, exc)
			raise

	# ... å…¶é¤˜å…§éƒ¨æ–¹æ³• (ä¿ç•™åŸæ¨£) ...

	def _index_exists(self) -> bool:  # pragma: no cover - simple passthrough
		"""Return True if index already exists.

		We call get_index; if it raises ResourceNotFoundError we return False.
		We intentionally avoid list_indexes for performance & RBAC scope.
		"""
		try:
			self.index_client.get_index(self.config.index_name)
			return True
		except ResourceNotFoundError:
			return False
		except HttpResponseError as exc:
			logger.warning("Index existence check failed (optimistically treat as absent): %s", exc)
			return False

	def _build_index(self) -> SearchIndex:  # type: ignore
		if SearchIndex is None:
			raise RuntimeError("SearchIndex type is not available. Ensure azure-search-documents SDK is installed.")
		fields = [
			SimpleField(name="id", type=SearchFieldDataType.String, key=True),
			SearchableField(
				name="title",
				type=SearchFieldDataType.String,
				analyzer_name=self.config.analyzer_name,
			),
			SearchableField(
				name="content",
				type=SearchFieldDataType.String,
				analyzer_name=self.config.analyzer_name,
			),
			SearchField(
				name=self.config.vector_field_name,
				type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
				searchable=True,
				vector_search_dimensions=self.config.embedding_dimensions,
				vector_search_profile_name="hnsw-profile",
			),
			SimpleField(name="category", type=SearchFieldDataType.String, filterable=True, facetable=True),
			SimpleField(
				name="tags",
				type=SearchFieldDataType.Collection(SearchFieldDataType.String),
				filterable=True,
				facetable=True,
			),
			# NOTE: we keep metadata_json for backwards compatibility.
			# If you need to filter/facet on metadata, consider splitting into dedicated fields.
			SimpleField(name="metadata_json", type=SearchFieldDataType.String),
			SimpleField(name="created_at", type=SearchFieldDataType.DateTimeOffset, filterable=True, sortable=True),
		]

		vector_search = VectorSearch(
			algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
			profiles=[
				VectorSearchProfile(
					name="hnsw-profile",
					algorithm_configuration_name="hnsw-config",
				)
			],
		)

		semantic_settings = None
		if self.config.use_semantic_search:
			semantic_settings = SemanticSearch(
				configurations=[
					SemanticConfiguration(
						name=self.config.semantic_configuration_name,
						prioritized_fields=SemanticPrioritizedFields(
							title_field=SemanticField(field_name="title"),
							content_fields=[SemanticField(field_name="content")],
							keywords_fields=[SemanticField(field_name="category")],
						),
					)
				]
			)

		index = SearchIndex(
			name=self.config.index_name,
			fields=fields,
			vector_search=vector_search,
			semantic_search=semantic_settings,
		)
		return index

	def index_document(
		self,
		doc_id: str,
		title: str,
		content: str,
		*,
		metadata: Optional[Dict[str, Any]] = None,
		category: Optional[str] = None,
		tags: Optional[Iterable[str]] = None,
		vector: Optional[Sequence[float]] = None,
		created_at: Optional[datetime] = None,
	) -> None:
		"""Upload or update a single document in the Azure AI Search index."""
		document: Dict[str, Any] = {
			"id": doc_id,
			"title": title,
			"content": content,
			"metadata_json": json.dumps(metadata or {}),
		}
		if category:
			document["category"] = category
		if tags:
			document["tags"] = list(tags)
		if created_at:
			document["created_at"] = created_at

		if vector is not None:
			# validate embedding dimension
			if len(vector) != self.config.embedding_dimensions:
				raise ValueError(
					f"Embedding vector length {len(vector)} != expected {self.config.embedding_dimensions}"
				)
			# coerce to floats
			document[self.config.vector_field_name] = [float(x) for x in vector]

		try:
			results = self.search_client.upload_documents(documents=[document])
		except HttpResponseError as exc:
			logger.error("Failed to index document %s: %s", doc_id, exc)
			raise

		failed = [item for item in results if not item.succeeded]
		if failed:
			error_message = failed[0].error_message or "Unknown Azure AI Search failure."
			raise RuntimeError(f"Azure AI Search indexing failed: {error_message}")

	def delete_document(self, doc_id: str) -> None:
		"""Remove a document from the Azure AI Search index."""
		try:
			self.search_client.delete_documents(documents=[{"id": doc_id}])
		except HttpResponseError as exc:
			logger.error("Failed to delete document %s: %s", doc_id, exc)
			raise

	def search(
		self,
		*,
		query_text: Optional[str] = None,
		query_vector: Optional[Sequence[float]] = None,
		top_k: int = 5,
		filter: Optional[str] = None,
		semantic: Optional[bool] = None,
		select: Optional[Sequence[str]] = None,
	) -> List[Dict[str, Any]]:
		"""Execute a hybrid / vector / semantic search.

		Returns a list of dict items with keys: id, title, content, score, metadata, (optional) highlights.
		"""
		if not AZURE_SEARCH_AVAILABLE:  # pragma: no cover - defensive
			raise RuntimeError("azure-search-documents SDK not available")

		search_text = query_text or "*"  # Azure requires a non-empty search_text in most cases
		kwargs: Dict[str, Any] = {}

		if query_vector is not None:
			if len(query_vector) != self.config.embedding_dimensions:
				raise ValueError(
					f"Query vector length {len(query_vector)} != expected {self.config.embedding_dimensions}"
				)
				# If dimensions mismatch, fail early instead of API error.
			vector_query = VectorizedQuery(
				vector=[float(x) for x in query_vector],
				k_nearest_neighbors=top_k,
				fields=self.config.vector_field_name,
			)
			kwargs["vector_queries"] = [vector_query]

		# Decide semantic usage: explicit param wins, else config default.
		use_semantic = self.config.use_semantic_search if semantic is None else semantic
		if use_semantic:
			kwargs["query_type"] = "semantic"
			kwargs["semantic_configuration_name"] = self.config.semantic_configuration_name
			# When using semantic search we can ask for captions / highlights if needed in future.

		if filter:
			kwargs["filter"] = filter
		if select:
			kwargs["select"] = ",".join(select)

		try:
			results_iter = self.search_client.search(
				search_text=search_text,
				top=top_k,
				include_total_count=False,
				**kwargs,
			)
		except HttpResponseError as exc:  # pragma: no cover - network error path
			logger.error("Azure AI Search query failed: %s", exc)
			raise

		items: List[Dict[str, Any]] = []
		for r in results_iter:  # type: ignore
			# r behaves like a dict-like object.
			metadata = self._parse_metadata(r.get("metadata_json"))  # type: ignore
			score = r.get("@search.score", 0.0)  # type: ignore
			item: Dict[str, Any] = {
				"id": r.get("id"),  # type: ignore
				"title": r.get("title", ""),  # type: ignore
				"content": r.get("content", ""),  # type: ignore
				"score": float(score) if score is not None else 0.0,
				"metadata": metadata,
			}
			highlights = r.get("@search.highlights")  # type: ignore
			collected = self._collect_highlights(highlights)
			if collected:
				item["highlights"] = collected
			items.append(item)

		return items

	# ---- ä¹‹å¾Œè‹¥é‚„æœ‰ search / query / helper methods, ä¿æŒåŸæ¨£ ----

	@staticmethod
	def _parse_metadata(serialized: Optional[str]) -> Dict[str, Any]:
		if not serialized:
			return {}
		try:
			return json.loads(serialized)
		except json.JSONDecodeError:
			logger.debug("Failed to decode metadata JSON: %s", serialized)
			return {"raw": serialized}

	@staticmethod
	def _collect_highlights(raw_highlights: Any) -> Optional[List[str]]:
		if not raw_highlights or not isinstance(raw_highlights, dict):
			return None
		collected: List[str] = []
		for values in raw_highlights.values():
			if isinstance(values, list):
				collected.extend(values)
		return collected or None


# ------------------------------------------------------------
# CLI / å–®æª”åŸ·è¡Œæ”¯æ´
# ------------------------------------------------------------
if __name__ == "__main__":  # pragma: no cover - ä¾›ä½¿ç”¨è€…æœ¬åœ°æ“ä½œç¤ºä¾‹
	import argparse
	import os
	import sys
	from pathlib import Path
	import io

	# å˜—è©¦è¼‰å…¥å°ˆæ¡ˆæ ¹ç›®éŒ„ .envï¼ˆè‹¥ä½¿ç”¨è€…åªå»ºç«‹æª”æ¡ˆä½†æœª export ç’°å¢ƒè®Šæ•¸ï¼‰
	try:  # pragma: no cover - è‹¥ç„¡ dotenv ä»å¯é‹ä½œ
		from dotenv import load_dotenv  # type: ignore
	except Exception:  # pragma: no cover
		load_dotenv = None  # type: ignore

	def _auto_load_env():  # pragma: no cover - è¼•é‡è¼”åŠ©
		if load_dotenv is None:
			return
		# è‡ªåº•å‘ä¸Šå°‹æ‰¾ç¬¬ä¸€å€‹å« .env çš„ç›®éŒ„
		for p in Path(__file__).resolve().parents:
			candidate = p / ".env"
			if candidate.exists():
				load_dotenv(candidate, override=False)
				break

	_auto_load_env()

	def _upload_to_blob_simple(file_path: Path) -> tuple[str, dict]:
		"""ç°¡åŒ–ç‰ˆæœ¬ï¼šä¸Šå‚³æª”æ¡ˆåˆ° Azure Blob ä¸¦é€²è¡ŒåŸºæœ¬å…§å®¹æå–"""
		try:
			from azure.storage.blob import BlobServiceClient
			
			# ç²å– Azure ç’°å¢ƒè®Šæ•¸
			storage_name = os.getenv('AZURE_STORAGE_ACCOUNT_NAME')
			storage_key = os.getenv('AZURE_STORAGE_ACCOUNT_KEY')
			container_name = os.getenv('AZURE_STORAGE_CONTAINER_NAME', 'documents')
			
			if not all([storage_name, storage_key]):
				raise ValueError("ç¼ºå°‘ Azure Storage ç’°å¢ƒè®Šæ•¸ï¼šéœ€è¦ AZURE_STORAGE_ACCOUNT_NAME å’Œ AZURE_STORAGE_ACCOUNT_KEY")
			
			# 1. ä¸Šå‚³åˆ° Blob Storage
			blob_service = BlobServiceClient(
				account_url=f"https://{storage_name}.blob.core.windows.net",
				credential=storage_key
			)
			
			blob_name = f"{datetime.utcnow().strftime('%Y/%m/%d')}/{file_path.name}"
			blob_client = blob_service.get_blob_client(container=container_name, blob=blob_name)
			
			with open(file_path, 'rb') as f:
				blob_client.upload_blob(f, overwrite=True)
			
			blob_url = blob_client.url
			print(f"ğŸŒ æª”æ¡ˆä¸Šå‚³åˆ° Blob: {blob_url}")
			
			# 2. åŸºæœ¬çš„æœ¬åœ°æ–‡å­—æå–
			content = ""
			if file_path.suffix.lower() == '.pdf':
				try:
					import fitz  # PyMuPDF
					doc = fitz.open(file_path)
					for page in doc:
						content += page.get_text()
					doc.close()
					print(f"âœ… PyMuPDF æˆåŠŸæå– {len(content)} å­—ç¬¦")
				except ImportError:
					print("âš ï¸  PyMuPDF æœªå®‰è£ï¼Œä½¿ç”¨æª”æ¡ˆåç¨±ä½œç‚ºå…§å®¹")
					content = f"PDF æª”æ¡ˆï¼š{file_path.name}"
			else:
				# å…¶ä»–æª”æ¡ˆé¡å‹ç›´æ¥è®€å–æ–‡å­—
				content = file_path.read_text(encoding="utf-8", errors="ignore")
				print(f"âœ… æ–‡å­—æª”æ¡ˆè®€å– {len(content)} å­—ç¬¦")
			
			metadata = {
				"blob_url": blob_url,
				"file_type": file_path.suffix.lower(),
				"file_size": file_path.stat().st_size,
				"processing_method": "blob_storage_simple"
			}
			
			return content, metadata
			
		except Exception as e:
			print(f"âŒ Azure Blob è™•ç†å¤±æ•—: {e}")
			raise



	def _handle_ingest_command(client: AzureAISearchExperimental, config: AzureSearchConfig, args) -> None:
		"""è™•ç† ingest å‘½ä»¤"""
		from pathlib import Path
		
		folder = Path(args.dir)
		if not folder.exists() or not folder.is_dir():
			raise SystemExit(f"è³‡æ–™å¤¾ä¸å­˜åœ¨æˆ–ä¸æ˜¯è³‡æ–™å¤¾: {folder}")
		
		# å¯é¸é‡å»º
		client.ensure_index(recreate=args.recreate)
		pattern = args.pattern
		files = list(folder.glob(pattern))
		if not files:
			print(f"ç„¡æª”æ¡ˆç¬¦åˆ {pattern} (è³‡æ–™å¤¾: {folder})")
			sys.exit(0)
		
		print(f"æ‰¾åˆ° {len(files)} å€‹æª”æ¡ˆï¼Œé–‹å§‹ä¸Šå‚³...")
		zero_vec = [0.0] * config.embedding_dimensions if args.zero_vector else None
		
		for i, f in enumerate(files, 1):
			try:
				file_extension = f.suffix.lower()
				
				# æ ¹æ“šåƒæ•¸é¸æ“‡è™•ç†æ–¹å¼
				if args.force_text or file_extension not in ['.pdf', '.txt', '.docx', '.html', '.md']:
					# å¼·åˆ¶æ–‡å­—è™•ç†æˆ–ä¸æ”¯æ´çš„æ ¼å¼
					print(f"[{i}/{len(files)}] ğŸ“„ ä½œç‚ºç´”æ–‡å­—æª”æ¡ˆè™•ç† {f.name}...")
					text = f.read_text(encoding="utf-8", errors="ignore")
					metadata = {"source_file": f.name, "processing_method": "text"}
				else:
					# ä½¿ç”¨ Azure Blob Storage ä¸Šå‚³ä¸¦ç°¡å–®æå–å…§å®¹
					print(f"[{i}/{len(files)}] ğŸŒ ä¸Šå‚³åˆ° Azure Blob {f.name}...")
					text, metadata = _upload_to_blob_simple(f)
				
				if not text.strip():
					print(f"[{i}/{len(files)}] {f.name} -> ç©ºå…§å®¹ï¼Œç•¥é")
					continue
				
				doc_id = f.stem
				client.index_document(
					doc_id=doc_id,
					title=f.name,
					content=text,
					metadata=metadata,
					category=args.category,
					tags=args.tags,
					vector=zero_vec,
					created_at=datetime.utcnow(),
				)
				print(f"[{i}/{len(files)}] ç´¢å¼•å®Œæˆ id={doc_id}")
				
			except Exception as e:  # pragma: no cover - ä½¿ç”¨è€…æª”æ¡ˆä¾‹å¤–
				print(f"[{i}/{len(files)}] {f.name} å¤±æ•—: {e}")
		
		print("æ‰¹æ¬¡ ingest å®Œæˆã€‚")
		print(f"å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æœå°‹ï¼špython {__file__} search --text 'æ‚¨çš„æŸ¥è©¢å­—ä¸²'")

	def _handle_upload_and_search_command(client: AzureAISearchExperimental, config: AzureSearchConfig, args) -> None:
		"""è™•ç† upload-and-search å‘½ä»¤"""
		from pathlib import Path
		
		file_path = Path(args.file)
		if not file_path.exists():
			raise SystemExit(f"æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
		
		print(f"=== é–‹å§‹è™•ç†æª”æ¡ˆ: {file_path.name} ===")
		
		# ç¢ºä¿ç´¢å¼•å­˜åœ¨
		client.ensure_index(recreate=False)
		
		try:
			file_extension = file_path.suffix.lower()
			doc_id = f"{file_path.stem}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
			
			# æ ¹æ“šåƒæ•¸é¸æ“‡è™•ç†æ–¹å¼
			if args.force_text or file_extension not in ['.pdf', '.txt', '.docx', '.html', '.md']:
				# å¼·åˆ¶æ–‡å­—è™•ç†æˆ–ä¸æ”¯æ´çš„æ ¼å¼
				print("ğŸ“„ ä½œç‚ºç´”æ–‡å­—æª”æ¡ˆè™•ç†...")
				text = file_path.read_text(encoding="utf-8", errors="ignore")
				metadata = {"source_file": file_path.name, "processing_method": "text"}
				print(f"âœ… æ–‡å­—æª”æ¡ˆè®€å–æˆåŠŸï¼Œå…§å®¹é•·åº¦: {len(text)} å­—ç¬¦")
			else:
				# ä½¿ç”¨ Azure Blob Storage ä¸Šå‚³ä¸¦ç°¡å–®æå–å…§å®¹
				print(f"ğŸŒ ä¸Šå‚³åˆ° Azure Blob {file_extension.upper()} æª”æ¡ˆ...")
				text, metadata = _upload_to_blob_simple(file_path)
				if text:
					print(f"âœ… æª”æ¡ˆè™•ç†æˆåŠŸï¼Œæå–å…§å®¹é•·åº¦: {len(text)} å­—ç¬¦")
					if metadata.get('blob_url'):
						print(f"ğŸŒ Blob URL: {metadata['blob_url']}")
				else:
					raise ValueError(f"{file_extension.upper()} æª”æ¡ˆè™•ç†å¤±æ•—ï¼Œç„¡æ³•æå–å…§å®¹")
			
			if not text.strip():
				raise ValueError("æª”æ¡ˆå…§å®¹ç‚ºç©º")
			
			# ä¸Šå‚³åˆ°ç´¢å¼•
			print(f"ğŸ“¤ å°‡æ–‡ä»¶ä¸Šå‚³åˆ° Azure AI Search (ID: {doc_id})...")
			client.index_document(
				doc_id=doc_id,
				title=file_path.name,
				content=text,
				metadata=metadata,
				category=args.category,
				tags=args.tags,
				vector=None,  # è®“ç³»çµ±è‡ªå‹•ç”Ÿæˆ embedding
				created_at=datetime.utcnow(),
			)
			print("âœ… æ–‡ä»¶å·²æˆåŠŸç´¢å¼•åˆ° Azure AI Search")
			
			# ç­‰å¾…ä¸€å°æ®µæ™‚é–“ç¢ºä¿ç´¢å¼•å®Œæˆ
			import time
			print("â³ ç­‰å¾…ç´¢å¼•å®Œæˆ...")
			time.sleep(2)
			
			# åŸ·è¡Œæœå°‹
			print(f"ğŸ” åŸ·è¡Œæœå°‹æŸ¥è©¢: '{args.query}'")
			results = client.search(
				query_text=args.query,
				top_k=5,
				semantic=config.use_semantic_search
			)
			
			print(f"\n=== æœå°‹çµæœ ({len(results)} ç­†) ===")
			if results:
				for i, r in enumerate(results, 1):
					print(f"\n[{i}] æ–‡ä»¶ ID: {r['id']}")
					print(f"    æ¨™é¡Œ: {r['title']}")
					print(f"    ç›¸é—œæ€§åˆ†æ•¸: {r['score']:.4f}")
					print(f"    å…§å®¹æ‘˜è¦: {r['content'][:200]}...")
					if r.get('highlights'):
						print("    é‡é»æ¨™è¨˜:")
						for h in r['highlights']:
							print(f"      - {h}")
					if r.get('metadata'):
						metadata_info = r['metadata']
						if isinstance(metadata_info, dict) and metadata_info.get('blob_url'):
							print(f"    Blob URL: {metadata_info['blob_url']}")
			else:
				print("âŒ æœªæ‰¾åˆ°ç›¸ç¬¦çš„çµæœ")
				print("ğŸ’¡ å»ºè­°ï¼š")
				print("   - æª¢æŸ¥æŸ¥è©¢é—œéµå­—æ˜¯å¦æ­£ç¢º")
				print("   - å˜—è©¦ä½¿ç”¨æ–‡ä»¶ä¸­çš„å…¶ä»–é—œéµå­—")
				print("   - ç¢ºèªæ–‡ä»¶å…§å®¹æ˜¯å¦åŒ…å«ç›¸é—œè³‡è¨Š")
			
			print(f"\n=== å®Œæˆï¼æ–‡ä»¶ ID: {doc_id} ===")
			
		except Exception as e:
			print(f"âŒ è™•ç†å¤±æ•—: {e}")
			raise SystemExit(1)

	def build_parser() -> argparse.ArgumentParser:
		parser = argparse.ArgumentParser(
			prog="azure_ai_search",
			description="AzureAISearchExperimental å–®æª”æ“ä½œï¼šå»ºç«‹ç´¢å¼• / ä¸Šå‚³æ–‡ä»¶ / æœå°‹ / åˆªé™¤ / demo",
		)
		sub = parser.add_subparsers(dest="command", required=True)

		# init / recreate index
		p_init = sub.add_parser("init", help="å»ºç«‹ç´¢å¼•ï¼›è‹¥å·²å­˜åœ¨å‰‡è·³é")
		p_init.add_argument("--recreate", action="store_true", help="è‹¥å­˜åœ¨å‰‡åˆªé™¤å¾Œé‡å»º")

		# index document
		p_index = sub.add_parser("index", help="ä¸Šå‚³æˆ–æ›´æ–°å–®ä¸€æ–‡ä»¶")
		p_index.add_argument("--id", required=True, help="æ–‡ä»¶ ID")
		p_index.add_argument("--title", required=True, help="æ¨™é¡Œ")
		p_index.add_argument("--content", required=True, help="å…§å®¹æ–‡å­—")
		p_index.add_argument("--category", help="åˆ†é¡ (å¯åš filter)")
		p_index.add_argument("--tags", nargs="*", help="æ¨™ç±¤ (å¤šå€¼)")
		p_index.add_argument("--metadata", help="JSON æ ¼å¼çš„é¡å¤–ä¸­ç¹¼è³‡æ–™ï¼Œä¾‹å¦‚ '{\"author\": \"me\"}'")
		p_index.add_argument("--zero-vector", action="store_true", help="ä½¿ç”¨å…¨ 0 å‘é‡ (åƒ…æ¸¬è©¦ç”¨)")

		# search
		p_search = sub.add_parser("search", help="åŸ·è¡Œæœå°‹ (æ–‡å­— / å‘é‡ / æ··åˆ)")
		p_search.add_argument("--text", help="ç´”æ–‡å­—æŸ¥è©¢å­—ä¸²")
		p_search.add_argument("--zero-vector", action="store_true", help="é™„å¸¶ä¸€å€‹å…¨ 0 æŸ¥è©¢å‘é‡ (æ¨¡æ“¬æ··åˆæŸ¥è©¢)")
		p_search.add_argument("--top", type=int, default=5, help="å›å‚³ç­†æ•¸ (default=5)")
		p_search.add_argument("--semantic", action="store_true", help="å•Ÿç”¨èªæ„æœå°‹")
		p_search.add_argument("--filter", help="OData filter æ¢ä»¶")

		# delete
		p_delete = sub.add_parser("delete", help="åˆªé™¤æ–‡ä»¶")
		p_delete.add_argument("--id", required=True, help="æ–‡ä»¶ ID")

		# demo: å…¨æµç¨‹
		sub.add_parser("demo", help="ç¤ºç¯„ï¼šå»ºç«‹ç´¢å¼• -> ä¸Šå‚³ä¸€ç­† sample -> æœå°‹ -> åˆªé™¤")

		# env: é¡¯ç¤ºé—œéµç’°å¢ƒè®Šæ•¸ (é™¤éŒ¯ç”¨)
		p_env = sub.add_parser("env", help="é¡¯ç¤ºç›®å‰è¼‰å…¥çš„é—œéµ Azure ç›¸é—œç’°å¢ƒè®Šæ•¸ (é®ç½©é‡‘é‘°)")

		# ingest: æ‰¹æ¬¡å°‡æŸè³‡æ–™å¤¾ (é è¨­ test_files) åº•ä¸‹çš„æ–‡å­—æª”åŠ å…¥ç´¢å¼•
		p_ingest = sub.add_parser("ingest", help="æ‰¹æ¬¡ä¸Šå‚³è³‡æ–™å¤¾ä¸­æ–‡ä»¶ (é è¨­æƒæ *.txt, æ”¯æ´ *.pdf)")
		p_ingest.add_argument("--dir", default="test_files", help="ä¾†æºè³‡æ–™å¤¾ï¼Œé è¨­ test_files")
		p_ingest.add_argument("--pattern", default="*.txt", help="æª”æ¡ˆåŒ¹é… (glob)ï¼Œé è¨­ *.txtï¼Œä¹Ÿå¯ç”¨ *.pdf")
		p_ingest.add_argument("--recreate", action="store_true", help="é–‹å§‹å‰é‡å»ºç´¢å¼•")
		p_ingest.add_argument("--zero-vector", action="store_true", help="ç‚ºæ¯å€‹æ–‡ä»¶é™„åŠ å…¨ 0 å‘é‡ (åƒ…æ¸¬è©¦ç”¨)")
		p_ingest.add_argument("--category", default="ingested", help="æ‰¹æ¬¡æ–‡ä»¶çš„ category æ¬„ä½å€¼")
		p_ingest.add_argument("--tags", nargs="*", default=["bulk"], help="æ‰¹æ¬¡æ–‡ä»¶çš„ tags (é è¨­ ['bulk'])")
		p_ingest.add_argument("--force-text", action="store_true", help="å¼·åˆ¶å°‡æ‰€æœ‰æª”æ¡ˆä½œç‚ºç´”æ–‡å­—è™•ç†ï¼ˆä¸ä¸Šå‚³åˆ° Azure Blobï¼‰")

		# upload-and-search: ä¸Šå‚³å–®ä¸€æª”æ¡ˆä¸¦ç«‹å³æœå°‹
		p_upload_search = sub.add_parser("upload-and-search", help="ä¸Šå‚³æª”æ¡ˆåˆ° Azure ä¸¦åŸ·è¡Œæœå°‹æ¸¬è©¦")
		p_upload_search.add_argument("--file", required=True, help="è¦ä¸Šå‚³çš„æª”æ¡ˆè·¯å¾‘")
		p_upload_search.add_argument("--query", required=True, help="æœå°‹æŸ¥è©¢å­—ä¸²")
		p_upload_search.add_argument("--force-text", action="store_true", help="å¼·åˆ¶ä½œç‚ºç´”æ–‡å­—è™•ç†ï¼ˆä¸ä¸Šå‚³åˆ° Azure Blobï¼‰")
		p_upload_search.add_argument("--category", default="test", help="æ–‡ä»¶åˆ†é¡")
		p_upload_search.add_argument("--tags", nargs="*", default=["test"], help="æ–‡ä»¶æ¨™ç±¤")

		return parser

	def load_config() -> AzureSearchConfig:
		service_name = os.getenv("AZURE_SEARCH_SERVICE_NAME")
		if not service_name:
			raise SystemExit("ç’°å¢ƒè®Šæ•¸ AZURE_SEARCH_SERVICE_NAME æœªè¨­å®š")
		api_key = os.getenv("AZURE_SEARCH_API_KEY")  # å¯ç‚º None => ä½¿ç”¨ DefaultAzureCredential
		index_name = os.getenv("AZURE_SEARCH_INDEX_NAME", "documents")
		use_semantic = os.getenv("AZURE_SEARCH_USE_SEMANTIC", "true").lower() in {"1", "true", "yes"}
		return AzureSearchConfig(
			service_name=service_name,
			api_key=api_key,
			index_name=index_name,
			use_semantic_search=use_semantic,
		)

	parser = build_parser()
	args = parser.parse_args()

	config = load_config()
	client = AzureAISearchExperimental(config)

	if args.command == "init":
		client.ensure_index(recreate=args.recreate)
		print(f"Index '{config.index_name}' ready (recreate={args.recreate})")

	elif args.command == "index":
		metadata = {}
		if args.metadata:
			try:
				metadata = json.loads(args.metadata)
			except json.JSONDecodeError as e:
				raise SystemExit(f"metadata ä¸æ˜¯åˆæ³• JSON: {e}")
		vector = None
		if args.zero_vector:
			vector = [0.0] * config.embedding_dimensions
		client.index_document(
			doc_id=args.id,
			title=args.title,
			content=args.content,
			metadata=metadata,
			category=args.category,
			tags=args.tags,
			vector=vector,
			created_at=datetime.utcnow(),
		)
		print(f"Indexed document id={args.id}")

	elif args.command == "search":
		query_vector = [0.0] * config.embedding_dimensions if args.zero_vector else None
		results = client.search(
			query_text=args.text,
			query_vector=query_vector,
			top_k=args.top,
			filter=args.filter,
			semantic=args.semantic or None,
		)
		print(f"Got {len(results)} results:")
		for i, r in enumerate(results, 1):
			print(f"[{i}] id={r['id']} score={r['score']:.4f} title={r['title']!r}")
			if r.get('highlights'):
				print("    highlights:")
				for h in r['highlights']:
					print(f"      - {h}")

	elif args.command == "delete":
		client.delete_document(args.id)
		print(f"Deleted document id={args.id}")

	elif args.command == "demo":
		print("[1] ç¢ºä¿ç´¢å¼•å­˜åœ¨ (è‹¥ä¸å­˜åœ¨å‰‡å»ºç«‹)...")
		client.ensure_index(recreate=False)
		doc_id = "demo-doc-001"
		print("[2] ä¸Šå‚³ç¤ºä¾‹æ–‡ä»¶ demo-doc-001 ...")
		client.index_document(
			doc_id=doc_id,
			title="ç¤ºä¾‹æ–‡ä»¶",
			content="é€™æ˜¯ä¸€å€‹ç¤ºç¯„æ–‡ä»¶å…§å®¹ï¼Œç”¨æ–¼ Azure AI Search å–®æª”åŸ·è¡Œ demoã€‚",
			metadata={"purpose": "demo"},
			category="demo",
			tags=["example", "demo"],
			vector=[0.0] * config.embedding_dimensions,
			created_at=datetime.utcnow(),
		)
		print("[3] åŸ·è¡Œæœå°‹ text='ç¤ºä¾‹' ...")
		results = client.search(query_text="ç¤ºä¾‹", top_k=3)
		for r in results:
			print(f"- {r['id']} score={r['score']:.4f} title={r['title']}")
		print("[4] æ¸…ç†åˆªé™¤ç¤ºä¾‹æ–‡ä»¶ ...")
		client.delete_document(doc_id)
		print("Demo å®Œæˆã€‚")

	elif args.command == "env":
		# é®ç½©é‡‘é‘°è¼¸å‡ºï¼Œä¾›ä½¿ç”¨è€…ç¢ºèªæ˜¯å¦æˆåŠŸè¼‰å…¥ .env
		def mask(v: str | None, keep: int = 4):
			if not v:
				return "<unset>"
			return v[:keep] + "***" if len(v) > keep else "***"
		keys = [
			"AZURE_SEARCH_SERVICE_NAME",
			"AZURE_SEARCH_API_KEY", 
			"AZURE_SEARCH_INDEX_NAME",
			"AZURE_SEARCH_USE_SEMANTIC",
			"AZURE_OPENAI_API_KEY",
			"AZURE_OPENAI_ENDPOINT",
			"AZURE_STORAGE_ACCOUNT_NAME",
			"AZURE_STORAGE_ACCOUNT_KEY",
			"AZURE_STORAGE_CONTAINER_NAME", 
			"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT",
			"AZURE_DOCUMENT_INTELLIGENCE_KEY",
		]
		print("ç›®å‰ä¸»è¦ç’°å¢ƒè®Šæ•¸ï¼š")
		for k in keys:
			print(f"  {k} = {mask(os.getenv(k))}")
		sys.exit(0)

	elif args.command == "ingest":
		_handle_ingest_command(client, config, args)

	elif args.command == "upload-and-search":
		_handle_upload_and_search_command(client, config, args)

	else:  # defensive
		parser.print_help()
		raise SystemExit(1)


